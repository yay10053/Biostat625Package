---
title: "Introduction to Lazy Modeling (LM) Function Package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to Linear Regression Function Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```



To start off your statistical journey, you will encounter some commonly used regression models, this package is here to guide you through this. There is 3 main functions and 1 helper function in this package:  

* SLM() :Simple Linear Regression  
* MultiLinearRegression() :Multiple Linear Regression  
* OnePassCov() :Covariance   
* RidgeRegression() :Ridge Regression  

This documentation will guide you through the detail of them one by one.  


```{r setup}
if (!require("bench")) {
    install.packages("bench")
}

if (!require("MASS")) {
    install.packages("MASS")
}

library(LM)
library(knitr)
library(bench)
```

# Simple Linear Regression with SLM()

SLM() allows you to perform simple linear regression with two main argument x and y, which are numeric vector.   

Given the simple linear regression: $y = x \beta + intercept + \epsilon$   

The function will return a list containing:   

* intercept: A numeric value of the intercept for $y = x \beta + intercept$   
* beta.est A numeric value of estimated beta for $y = x \beta + intercept$   
* Std.Error A numeric value of standard error   
* t.value A numeric value of t-statistic    
* p.value A numeric value of p-values   

## SLM() Example

Suppose you would like to investigate the association of the weight of the vehicle (wt) and fuel efficiency (mpg) in the mtcars dataset. Assuming all linear regression assumptions hold, then you have:  

$mpg = wt \beta + intercept +\epsilon$  


```{r SLM example}
##Load dataset
library(datasets)
data(mtcars)
x <- mtcars$wt
y <- mtcars$mpg

fit = SLM(x,y)
fit
```

## SLM() Accurancy and Efficiency Evalution

The function SLM() can be compare with the lm() function in the Base package.   

```{r SLM acc}
b <- summary(lm(y~x))
lmfit <- c(b$coefficients[1,1], b$coefficients[2,])
tab <- cbind(unlist(fit), lmfit)
colnames(tab) <- c("Outputs from LM", "Outputs from BASE")
kable(tab, caption = "Simple Linear Regression comparison: LM vs BASE")
```

Since the function SLM() is coded with Rcpp, the efficiency of the function is improved. The SLM() with the RCPP is as efficient as the lm() from the base package.   

```{r SLM Eff, fig.height = 3, fig.width = 10, fig.align = "center"}
LM.SLM <- unlist(SLM(x, y))
Base.lm <-  c(b$coefficients[1,1], b$coefficients[2,])
all.equal(LM.SLM,Base.lm, check.names = F)

p <- mark(unname(LM.SLM),unname(Base.lm))
plot(p)


```


# Multiple Linear Regression with MultiLinearRegression()

In case where we have more than one covariate, we would like to apply the function MultiLinearRegression(). This function utilize the Cholesky Decomposition to solve for the regression model.   

Given the multiple linear regression: $y_{i} =  \sum_{j=1}^{p} X_{i} \beta_{j}  + intercept + \epsilon$   

You will need to input:  

* x :A size n x p Design matrix for regression (without intercept)   
* y :A size n vector  
* int: If this linear regression has intercept? TRUE for has intercept, FALSE for no intercept  

The function will return a list containing:  

* beta.est :A vector of estimated beta for y ~ x *beta + error  
* Std.Error :A vector of standard error for each estimate  
* t.val :A vector of t-statistic for each estimate  
* p.value :A vector of p-values of each estimate  
* fitted :A vector of fitted values  
* SSE :A numeric value for the Sum of squared error of the model  
* R.square :A numeric value for the R square of the model  

## MultiLinearRegression() Example

Suppose you would like to investigate the association of the weight of the vehicle (wt) and fuel efficiency (mpg) in the mtcars dataset adjusting for performance measure (qsec) and number of cylinders (cyl). Assuming all linear regression assumptions hold, then you have:   

$mpg = wt * \beta_{1} + qsec*\beta_{2} + cyl*\beta_{3} + intercept + \epsilon$


```{r ML example}
##Load dataset
x <- as.matrix(mtcars[,c(6,7,2)])
y <- as.vector(mtcars$mpg)
fit = MultiLinearRegression(x,y,int = T)
fit
```
## MultiLinearRegression() Accurancy and Efficiency Evalution

The function MultiLinearRegression() can be compare with the lm() function in the Base package.      
Since there are too much variables, we will compare the estimates and the fitted values from both models . 

```{r ML acc}
b <- summary(lm(y~x))
lmfit <- c(b$coefficients[,1])
tab <- cbind(unlist(fit$beta.est), lmfit)
colnames(tab) <- c("estimates from LM", "estimates from BASE")
kable(tab, caption = "Multiple Linear Regression comparison: LM vs BASE")

paste0("The fitted value from both LM and Base model are the same: ", all.equal(unname(as.vector(fit$fitted)), unname(as.vector(lm(y~x)$fitted))))
```

## Comparing the efficiency of the MultiLinearRegression():

The MultiLinearRegression() run faster than the lm(), although it might not be a fair comparison as the lm() provided more information. Therefore, if you would like to calculate estimates with large data set with faster computational time. Using  MultiLinearRegression() would be a good option. 


```{r ML Eff, fig.height = 3, fig.width = 10, fig.align = "center"}
set.seed(123)
x <- matrix(rnorm(1:1000), 100, 10)
set.seed(321)
y <- rnorm(1:100)
p <- mark(MultiLinearRegression(x,y,int = T), lm(y~x), check = F)
plot(p)

```

# Covariance with OnePassCov()

This is a helper function in the package since we would often time interested in the covariance between two numeric vector as we are building regression model. This function used a one-pass technique to improve efficiency.    

Given the formula:   
$cov(x,y) =  1/(n-1) \sum_{i=1}^{n} (x_{i} - \overline{x}) + (y_{i} - \overline{y})$   

You will need to input:  
* x :A size n vector  
* y :A size n vector  

The function will return a numeric covariance between the two vectors

## OnePassCov() Example

```{r cov ex}
##Accuracy Evaluation of OnePassCov()
set.seed(123)
x <- rnorm(1:1000)
set.seed(321)
y <- rnorm(1:1000)
op = OnePassCov(x, y)
op
```
## OnePassCov() Accurancy and Efficiency Evalution

The function OnePassCov() can be compared with the cov() function in the Base package. 

```{r cov acc}
##Accuracy Evaluation of OnePassCov()
paste0("The covariance from both LM and Base are the same: ", all.equal(op, cov(x,y)))

```

## The efficiency of the OnePassCov():

comparing with the cov() from base package, the plot below shows that the OnePassCov() is not as fast as the cov() from the base package, but it is still efficient enough to handle large vectors calculation. 

```{r cov eff, fig.height = 3, fig.width = 10, fig.align = "center"}
p = mark(OnePassCov(x,y), cov(x,y))
plot(p)
```

# Ridge Regression with RidgeRegression()

RidgeRegression() allows you to perform ridge regression. 

You will need to input:  

* x :A size n x p Design matrix for regression (without intercept)  
* y :A size n vector  
* lambda : A numeric number, tuning parameter of the ridge regression model  
* int: If this Regression has intercept? TRUE for has intercept, FALSE for no intercept  

The function will return a vector of estimated beta (to 4 precision) of the ridge regression model

Given the multiple linear regression: $y_{i} =  \sum_{j=1}^{p} X_{i} \beta_{j}  + intercept + \epsilon$   
With ridge regression method, we minimize the penalized residual sum of squares: $(y - X\beta)^T(y - X\beta) + \lambda \beta^T \beta$   
where $\lambda > 0$


## RidgeRegression() Example

```{r ridge ex}
set.seed(123)
x <- matrix(rnorm(1:500), 100, 5)
set.seed(321)
y <- rnorm(1:100)
beta = RidgeRegression(x, y, lambda = 0.1, int = TRUE)
beta
```
## RidgeRegression() Accurancy and Efficiency Evalution

RidgeRegression function is comparable to the lm.ridge() function in MASS package with the 4 precision.

```{r ridge acc}
##Accuracy Evaluation of RidgeREgression()
library(MASS)
beta.ml <-  round(as.numeric(coefficients(lm.ridge(y~x, lambda = 0.1))),4)
mat <- cbind(beta,beta.ml)
colnames(mat) <- c("Estimates from LM", "Estimate from MASS")
kable(mat, caption = "Ridge Regression estimation comparison: LM vs MASS")
```

## Comparing the efficiency of RidgeRegression():

The RidgeRegression() run faster than the lm.ridge(), although it might not be a fair comparison as the lm.ridge() provided more information. Therefore, if you would like to calculate estimates with large data set with faster computational time. Using RidgeRegression() would be a good option. 

```{r ridge eff, fig.height = 3, fig.width = 10, fig.align = "center"}
p <- mark(RidgeRegression(x, y, lambda = 0.1, int = TRUE), lm.ridge(y~x, lambda = 0.1), check = F)
plot(p)
```

### *End of the Document*
